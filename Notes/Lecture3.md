#### 1. **Introduction to Language Models**

Language models assign probabilities to sequences of words, allowing them to predict upcoming words or evaluate the likelihood of a sentence. These models are essential in NLP applications such as speech recognition, machine translation, and text generation.

- **Objective**: To represent the probability of a sequence of words, $\( P(w_1, w_2, \ldots, w_n) \)$, to predict or generate text.
- **Applications**: Language models can be used in predictive text input, machine translation, sentiment analysis, and other NLP tasks.

#### 2. **N-gram Models as Language Models**

An **N-gram model** predicts a word based on the previous $\( N-1 \)$ words. It approximates the probability of a word sequence by breaking it down into smaller word combinations (N-grams).

- **Types of N-grams**:
   - **Unigram**: Probability of each word independently.
   - **Bigram**: Probability of each word given the previous word.
   - **Trigram**: Probability of each word given the two preceding words.

   For example, in a bigram model:
   $$\[
   P(w_1, w_2, \ldots, w_n) = P(w_1) \cdot P(w_2 | w_1) \cdot P(w_3 | w_2) \cdots P(w_n | w_{n-1})
   \]$$
This approach simplifies calculations but has limitations due to fixed context length and data sparsity.

#### 3. **Challenges with N-gram Models**

##### 3.1 Data Sparsity
The likelihood of encountering certain N-grams is low, especially in smaller datasets, which can result in zero probabilities for many valid word sequences.

##### 3.2 Long-Range Dependencies
N-grams are limited by their window size (N), which means they may not capture dependencies between words that are far apart in a sentence, leading to a loss of context and coherence in prediction.

##### 3.3 Out-of-Vocabulary (OOV) Words
N-grams struggle when they encounter words that were not seen in the training data. Such words are assigned zero probability, affecting the overall prediction quality.

#### 4. **Handling OOV Words and Data Sparsity**

To address the zero-probability issue and sparse data:
   - **Lexicon Creation**: Define a set of frequent words as the lexicon; rare words are replaced by an "unknown" token (UNK).
   - **UNK Token**: Maps all words outside the lexicon to the UNK token, smoothing out the probability distribution and reducing zero-probability issues for unseen words during testing.

#### 5. **Smoothing Techniques in Language Models**

To mitigate the problem of zero probabilities and improve the robustness of language models, several smoothing techniques are used:

##### 5.1 Add-One (Laplace) Smoothing
Adds one to the count of each N-gram to ensure all possible sequences have non-zero probabilities:
   $$\[
   P(w_n | w_{n-1}) = \frac{C(w_{n-1}, w_n) + 1}{C(w_{n-1}) + V}
   \]$$
where \( V \) is the vocabulary size, and \( C \) is the count of occurrences. While easy to implement, Add-One smoothing can overly reduce the probability of frequent sequences.

##### 5.2 Add-K Smoothing
A variation of Add-One smoothing that allows a smaller constant \( K \) (between 0 and 1), reducing the effect on frequent N-grams:
   $$\[
   P(w_n | w_{n-1}) = \frac{C(w_{n-1}, w_n) + K}{C(w_{n-1}) + KV}
   \]$$
Add-K smoothing provides more control, particularly in balancing between rare and frequent words.

#### 6. **Backoff and Interpolation Techniques**

##### 6.1 Backoff Smoothing
Backoff smoothing applies a lower-order model if the higher-order modelâ€™s probability is zero:
   - Example: If $\( P(w_n | w_{n-1}, w_{n-2}) = 0 \)$, the model falls back to $\( P(w_n | w_{n-1}) \)$, and if that also fails, it falls back to the unigram probability \( P(w_n) \).

Backoff provides flexibility in cases where higher-order N-grams are unavailable, maintaining the prediction flow without breaking.

##### 6.2 Interpolation Smoothing
Interpolation combines probabilities from different N-gram levels (e.g., trigram, bigram, and unigram) by assigning weights to each model:
   $$\[
   P(w_n | w_{n-1}, w_{n-2}) = \lambda_3 P(w_n | w_{n-1}, w_{n-2}) + \lambda_2 P(w_n | w_{n-1}) + \lambda_1 P(w_n)
   \]$$
where $\( \lambda_1 + \lambda_2 + \lambda_3 = 1 \)$. Interpolation ensures that each N-gram level contributes, enhancing prediction by leveraging both local and global context.

#### 7. **Advanced Smoothing Techniques**

Advanced techniques like Good-Turing and Kneser-Ney provide refined smoothing by adjusting for patterns in word occurrence frequencies and reassigning probabilities to low-frequency events:

##### 7.1 Good-Turing Smoothing
Good-Turing adjusts probabilities based on the number of N-grams occurring once or rarely, redistributing probability mass from seen to unseen events. This method is highly useful in balancing between common and rare events.

##### 7.2 Kneser-Ney Smoothing
Kneser-Ney smoothing enhances predictions by considering how often words appear in diverse contexts, increasing the probability of less frequent but contextually significant words over common filler words. This technique is especially effective in language models where context-driven word choice is critical.

#### 8. **Evaluating Language Models with Perplexity**

Perplexity is the standard metric for evaluating the effectiveness of language models. It measures how well a model predicts a test set by calculating the inverse probability of the test data, normalized by the number of words.

- **Formula**:
   $$\[
   \text{Perplexity} = 2^{H(M)}
   \]$$
   where $\( H(M) \)$ is the cross-entropy of the model. Lower perplexity indicates better predictive performance.

---

##### Importance of Perplexity
A low perplexity score suggests that a language model is accurate and efficient at predicting the likelihood of sentences. It serves as a benchmark for comparing models and assessing improvements.

#### 9. **Limitations of Traditional Language Models and Shift to Neural Models**

While N-gram models provide a foundational approach to language modeling, they have limitations:
   - **Fixed-Length Contexts**: N-gram models struggle with long-range dependencies beyond the N-length window.
   - **Data Sparsity**: As N increases, data sparsity grows, making high-order N-grams unreliable.
   - **Contextual Limitations**: Cannot effectively capture nuanced relationships between distant words.

##### Shift to Neural Models
With advancements in deep learning, **Recurrent Neural Networks (RNNs)**, **Long Short-Term Memory networks (LSTMs)**, and **Transformer models** offer alternatives that capture long-range dependencies and complex language structures without needing explicit smoothing. Neural models generalize better, manage larger vocabularies, and adapt to more dynamic contexts, overcoming many of the limitations of traditional N-gram approaches.

---

### Summary

In this lecture, we explored the foundations of language models, focusing on N-gram models, smoothing techniques, and probability estimation. Key takeaways include:

1. **N-gram Models**: Simple yet powerful, N-grams capture short-term dependencies, but struggle with long-range context and data sparsity.
2. **Smoothing Techniques**: Approaches like Add-One, Add-K, backoff, interpolation, Good-Turing, and Kneser-Ney smooth out probabilities, helping models handle unseen events and rare word patterns.
3. **Model Evaluation with Perplexity**: Perplexity serves as a key metric for assessing model accuracy, with lower values indicating stronger predictive capabilities.
4. **Transition to Neural Models**: The limitations of traditional models have spurred the adoption of neural network-based language models, which capture language nuances more effectively.

Overall, language models are essential in NLP, providing the statistical foundation for numerous applications. As models continue to evolve, the shift toward neural architectures opens new frontiers for understanding and generating human language.
